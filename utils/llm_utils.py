import requests

def send_prompt_to_llm(prompt, api_url="http://127.0.0.1:1234"):
    """
    Sends a prompt to the LLM and returns the response.

    Args:
        prompt (str): The prompt to send to the LLM.
        api_url (str): The base URL of the LLM API.

    Returns:
        str: The response content from the LLM.

    Raises:
        Exception: If the API request fails or no response is generated.
    """
    payload = {
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.8,
        "top_p": 0.95,
        "max_tokens": 4096
    }

    try:
        print("ðŸ§  Sending prompt to LLM...")
        response = requests.post(f"{api_url}/v1/chat/completions", json=payload, timeout=300)

        if response.status_code != 200:
            raise Exception(f"API request failed: {response.status_code} - {response.text}")

        response_content = response.json().get("choices", [{}])[0].get("message", {}).get("content", "").strip()
        if not response_content:
            raise ValueError("No response generated by the model.")
        return response_content

    except requests.exceptions.Timeout:
        raise Exception("The request timed out. Try increasing the timeout or optimizing the prompt.")
    except Exception as e:
        raise Exception(f"An error occurred: {e}")